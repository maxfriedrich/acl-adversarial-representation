% !TeX root=main
% !TeX spellcheck=en_US

\section{Adversarial Representation}\label{sec:adversarial-representation}
%
The previously discussed approaches are highly dependent on manually tuned representation parameters like the amount of noise and neighbor space, that, if set incorrectly, may allow for easy re-identification.
%
In this experiment, we evaluate an adversarial learning based approach that automatically tunes the representation parameters to protect against adversary models.

\begin{description}
    \item[Architecture]
    %
    Our approach uses a model that is composed of three components: a representation model, a de-identification model, and an adversary.
    %
    An overview of the architecture is shown in \cref{fig:adversarial-model}.
    %
    The representation model maps a sequence of word embeddings to an intermediate vector representation sequence.
    %
    The de-identification model receives this representation sequence as an input instead of the original embedding sequence.
    %
    It retains the casing feature as an additional input.
    %
    As before, the de-identification model outputs a sequence of class probabilities.
    %
    The representation is also used as an input to the adversary that tries to infer information about the original embedding sequence.
    
    \begin{figure}
        \centering
        \input{images/tikz/adversarial}
        \caption[Adversarial model architecture]{%
            Simplified visualization of the adversarial model architecture.
            %
            Sequences of squares denote real-valued vectors, dotted arrows represent possible additional real or fake inputs to the adversary.
            %
            The casing feature that is provided as a second input to the de-identification model is omitted for legibility.}\label{fig:adversarial-model}
    \end{figure}
    
    \item[Representations]
    %
    We evaluate two types of representation models: a feedforward and \iac{lstm} model.
    %
    Both apply Gaussian noise with zero mean and trainable standard deviations to their inputs and outputs.
    %
    The models learn a standard deviation for each of the input and output dimensions.
    
    %
    We try different representation sizes to explore the trade-off between de-identification and adversary performances.
    %
    In contrast to the approaches from \cref{sec:perturbing} that only perturb \ac{phi} tokens, the representation models in this approach process all tokens to represent them in a new embedding space.
    
    \item[Adversaries]
    %
    In existing gradient reversal approaches \citep{ganin2016domain,feutry2018learning,elazar2018adversarial}, the learned representation is invariant to some attribute of the input.
    %
    Similarly, our representation should be invariant to small input changes, like a single token being replaced with a neighbor in the embedding space.
    %
    The number of neighbors $N$ controls the privacy properties of the representation.
    
    %
    Additionally, we need our representation to contain a random element because we want to share the output representations as well as the representation model itself.
    %
    An attacker should not be able to create a lookup table of representations for exact sentences, i.e.\ the representation must be immune to known-plaintext attacks.
    
    %
    To achieve these goals, we use two adversaries that are trained for the following tasks:
    \begin{enumerate}
        \item Given a representation and an embedding sequence, decide if they were obtained from the same sentence.
        \item Given two representation sequences (and their cosine similarities), decide if they were obtained from the same sentence.
    \end{enumerate}
    
    %
    \Cref{fig:adversaries} shows the two adversaries with their respective inputs.
    %
    The first adversary's objective is a discriminatory formulation of an inverse representation model and causes representations for similar inputs (replacing any protected token with one of its $N$ neighbors) to be indistinguishable.
    %
    The second adversary's objective causes repeated representation computations for the same sentence to differ by a high enough degree to make it impossible to build a lookup table of representations.
    %
    We obtain the representation sequences for the second adversary from copies of the representation model with shared weights.
    %
    We generate real and fake pairs for adversarial training using the automatic pseudonymization approach presented in \cref{sec:auto-pseudo}, limiting the number of replaced tokens to one per sentence.
    
    %
    The adversaries are implemented as bidirectional \ac{lstm} models.
    %
    We confirmed that bidirectional \ac{lstm} models are able to learn the adversarial tasks on randomly generated data and raw word embeddings in a preliminary experiment.
    %
    To use the two adversaries in our architecture, we average their outputs.
    
    \item[Training]
    %
    We evaluate two training procedures: \ac{dann} training~\citep{ganin2016domain} and the alternating approach by \citet{feutry2018learning}.
    
    %
    In \ac{dann} training, the three components are trained conjointly, optimizing the sum of losses.
    %
    Training the de-identification model modifies the representation model weights to generate a more meaningful representation for de-identification.
    %
    The adversary gradient is reversed with a gradient reversal layer between the adversary and the representation model in the backward pass, causing the representation to become less meaningful for the adversary.
    
    %
    The training procedure by \citet{feutry2018learning} is shown in \cref{fig:feutry-training}.
    %
    It is composed of three sequential phases:
    %
    \begin{enumerate}
        \item The de-identification and representation models are pre-trained together, optimizing the de-identification loss $L_{\text{deid}}$.
        \item The representation model is frozen and the adversary is pre-trained, optimizing the adversarial loss $L_{\text{adv}}$.
        \item In alternation, for one epoch each:
        \begin{enumerate}
            \item The representation is frozen and both de-identification model and adversary are trained, optimizing their respective losses $L_{\text{deid}}$ and $L_{\text{adv}}$.
            \item The de-identification model and adversary are frozen and the representation is trained, optimizing the combined loss $L_{\text{repr}} = L_{\text{deid}} + \lambda \abs{L_{\text{adv}} - L_{\text{random}}}$. \label{item:repr-training}
        \end{enumerate}
    \end{enumerate}
    
    \begin{figure}
        \centering
        \input{images/tikz/training}
        \caption[Adversarial training procedure]{%
            Visualization of \citeauthor{feutry2018learning}'s training procedure.
            %
            The adversarial model layout follows \cref{fig:adversarial-model}: the representation model is at the bottom, the left branch is the de-identification model and the right branch is the adversary.
            %
            In each step, the thick components are trained while the thin components are frozen.
            %
            Steps 1 and 2 are trained until stable.
            %
            Then training alternates between one epoch and step 3a and one epoch of step 3b.
        }\label{fig:feutry-training}
    \end{figure}
    
    %
    In the first two phases, we monitor the respective validation losses for early stopping to decide at which point the training should move on to the next phase.
    %
    The alternating steps in the third phase each last one training epoch.
    %
    We determine the early stopping epoch using only the combined validation loss (\cref{item:repr-training}).
    
    %
    Gradient reversal is achieved by optimizing the combined representation loss while the adversary weights are frozen.
    %
    The combined loss is motivated by the fact that the adversary performance should be the same as a random guessing model, which is a lower bound for anonymization~\citep{feutry2018learning}.
    %
    The term $\abs{L_{\text{adv}} - L_{\text{random}}}$ approaches $0$ when the adversary performance approaches random guessing\footnote{In the case of binary classification: $L_{\text{random}} = -\log \frac{1}{2} \approx 0.6931$.}.
    %
    $\lambda$ is a weighting factor for the two losses; we select $\lambda=1$.
    
    \item[Application]
    %
    To apply the model in practice, a central model provider would train the three parts of the model on an initial \ac{phi}-annotated dataset, e.g.\ the i2b2 2014 data.
    %
    This initial training should confirm that the learned representation allows training a de-identification model while being robust to the adversaries.
    %
    The model provider would then publish the representation model along with their choice of pre-trained word embeddings.
    %
    Medical institutions would use the representation model to transform their \ac{phi}-labeled data into a private representation, which is then sent back to the central model provider with the respective labels.
    %
    This transformation replaces the manual document-coherent pseudonymization that is typically performed to share training data for de-identification.
    
    %
    The model provider would then update the existing de-identification model or train a new model using all available representation data.
    %
    Periodically, the pipeline of representation model (possibly in a version without additive noise) and de-identification model would be published so it can be used by medical institutions on their unlabeled data.
\end{description}