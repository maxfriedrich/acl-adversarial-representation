% !TeX root=main
% !TeX spellcheck=en_US

\section{Discussion}\label{sec:discussion}

\subsection{De-Identification Performance}
%
In this section, we discuss the de-identification performance properties of our models.

\begin{description}
    \item[Baseline De-Identification]
    %
    Our \ac{elmo} model is the best de-identification model, followed by the FastText and GloVe models.
    %
    \Ac{elmo} embeddings encapsulate the sentence context, making them more expressive than the non-contextual embedding types.
    
    %
    \citet{reimers2017optimal} find that GloVe performs better than FastText in their \ac{ner} benchmark.
    %
    This is no contradiction to our result as the i2b2 2014 dataset in all likelihood contains more out-of-vocabulary tokens (that GloVe cannot embed) than their datasets.
    %
    FastText's approach to embedding unknown words (word embeddings are the sum of their subword embeddings) proves useful on datasets with misspellings and ungrammatical text.
    %
    However, FastText beats GloVe only by $0.05$ percentage points on the i2b2 test set.
    %
    The casing feature (which improves GloVe be $0.4$ percentage points in the hyperparameter optimization) makes up for GloVe's missing embeddings for unknown words.
    
    %
    Analyzing our baseline models' predictions on the test set reveals that the models typically make wrong predictions for the same instances of \ac{phi}.
    %
    \Cref{tab:deid-errors} shows some wrong predictions from each of our models that are unique to the respective model\footnote{We show similar sentences to actual wrongly predicted sentences from the i2b2 test set.}.
    %
    Most notably, the GloVe model makes mistakes that can be avoided by using subword features. 
    %
    Slight misspellings have embeddings close to the correct spelling in \ac{elmo} and FastText.
    %
    Also, entity names that clearly belong to a specific category (like city names with the suffix ``-town'') have meaningful embeddings in \ac{elmo} and FastText.
    %
    Some of the \ac{elmo} and FastText models' mistakes could be avoided by using a dictionary of professions, hospital names, and medical terms as an additional data source, e.g.\ ``Bruce Protocol'' is a standard cardiac diagnostic test that would occur in a medical dictionary.
\end{description}

\begin{description}
    \item[Automatically Pseudonymized Data]
    %
    Our naive automatic word-level pseudonymization approach allows training reasonable de-identification models when selecting from up to $N=500$ neighbors.
    
    %
    \Cref{fig:pseudonymizations} shows four examples for automatically pseudonymized sentences that were generated by moving \ac{phi} tokens to one of their $N=100$ neighbors in the FastText embedding space.
    %
    While the first three sentences are coherent and could be the result of a human pseudonymization step, the unrealistic last sentence hints at the limitations of this naive approach.
    %
    The token ``HMS'' was seemingly moved too far because ``Q-ship'' is not a realistic hospital name, and the tokens ``Florida'' and ``Hospital'' were only perturbed by a small degree.
    %
    However, as deep learning models do not see the actual words but only their representations (that are neighbors in the embedding space), using the automatically pseudonymized data does not dramatically deteriorate de-identification performance.
    %
    Rather, models trained on automatically pseudonymized data beat their counterparts that were trained on raw data in some categories of \ac{phi} due to their higher robustness.
    
    \begin{figure}
        % \small
        % \sffamily
        \begin{framed}
            \begin{itemize}[noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=*]
                \item \sout{Bob} \textbf{John} continues to feel well
                \item She is a fulltime \sout{historian} \textbf{photographer}
                \item TReated with RAI \sout{February} \textbf{September} \sout{2071} \textbf{2080}
                \item  Transferred in \sout{January} \textbf{October} to the \sout{HMS} \textbf{Q-ship} after presenting to the \sout{Florida} \textbf{Miami-Florida} \sout{Hospital} \textbf{Hosp} \sout{Orlando} \textbf{Merlina}
            \end{itemize}
        \end{framed}
        \caption[Automatically pseudonymized data.]{Examples of automatically pseudonymized sentences using FastText embeddings and $N=100$ neighbors.}\label{fig:pseudonymizations}
    \end{figure}
    
    \item[Adversarially Learned Representation]
    %
    Our adversarially trained vector representation that is invariant to word changes allows training reasonable de-identification models when using up to $N=1000$ neighbors as an invariance requirement.
    
    %
    The adversarial de-identification results beat the automatic pseudonymization results because the representation model can act as a task-specific feature extractor.
    %
    Additionally, the representations are more general as they are invariant to word changes.
    %
    The de-identification model is trained on sentences that are augmented to a smaller degree than in our automatic pseudonymization experiment (only one \ac{phi} token is moved to a neighbor in adversarial training instead of all \ac{phi} tokens).
    
\end{description}

\subsection{Privacy Properties}
%
In this section, we discuss the privacy properties of our approaches.

\begin{description}
    \item[Embeddings]
    %
    When looking up embedding space neighbors for words, it is notable that many FastText neighbors include the original word or parts of it as a subword.
    %
    This is due to FastText's method of using the sum of subword embeddings in embedding calculation.
    %
    For tokens that occur as \ac{phi} in the i2b2 training set, on average $7.37$ of their $N=100$ closest neighbors in the FastText embedding matrix contain the original token as a subword.
    %
    When looking up neighbors using GloVe embeddings, the value is $0.44$.
    %
    This may indicate that FastText requires stronger perturbation (i.e.\ higher $N$) than GloVe to sufficiently obfuscate protected information.
    
    \item[Automatically Pseudonymized Data]
    %
    We identified some privacy weaknesses of our automatic pseudonymization approach.
    %
    For a last name like \textit{Wolf}, neighbors in the embedding space will include other animal names and not common last names.
    %
    In this case, it could be possible to infer the original name if there are only a limited number of people that might appear in the dataset, e.g.\ the population of a small town.
    
    %
    If multiple sentences contain animal names (or any other similar names), they will likely come from the same original document, undoing the privacy gain from shuffling training sentences across documents.
    %
    It may be possible to infer the original name using the overlapping neighbor spaces.
    %
    To counter this, we can re-introduce document-level pseudonymization, i.e.\ moving all occurrences of \iac{phi} token to the same neighbor.
    %
    However, we would then also need to detect misspelled names as well as other hints to the actual tokens and transform them similarly to the original, which would add back much of the complexity of manual pseudonymization that we try to avoid.
    
    %
    In our adversarial evaluation, the adversaries reach test accuracies of $60\%$.
    %
    However, a $60\%$ accuracy is also reached by a similar \ac{lstm} adversary that is trained to discriminate original sequences from sequences with one occurrence of \ac{phi} moved to a neighbor.
    %
    This means that the adversary can achieve its accuracy only by learning to distinguish real from fake sentences and ignoring its pseudonymized sequence input.
    
    \item[Adversarially Learned Representation]
    %
    Our adversarial representation empirically satisfies a strong privacy criterion: representations are invariant to \textit{any} protected information token being replaced with \textit{any} of its $N$ neighbors in an embedding space.
    %
    While it is possible for de-identification models to achieve \fone scores of $95\%$ using our adversarially learned representation with up to $N=1000$ neighbors, training becomes unstable for large $N$.
    
    %
    \Cref{fig:adversarial-learning-curves} shows a set of typical (successful) learning curves for \citeauthor{feutry2018learning}'s training procedure.
    %
    The representation model and de-identification model are jointly pre-trained in the first phase.
    %
    In the second phase, the adversary is pre-trained to reach a validation accuracy of around $80\%$, which means that the pre-trained representation does not fit our invariance requirements.
    %
    At the beginning of the third training phase, both the de-identification model and the adversary learning curves show an oscillating pattern that is caused by the alternating training of the branches (with frozen representation model) and the representation model (with frozen branches).
    %
    Around 15 epochs into this phase, we find an adequate representation which is indicated by the adversary accuracy being close to the random guessing accuracy of $50\%$.
    %
    The de-identification validation loss is typically lower than the training loss in the alternating phase.
    %
    Since the representation and de-identification models are not trained together in this phase, the models always need to \textit{catch up} to the other model's parameter changes (training loss is averaged over all training samples of an epoch).
    %
    When freezing the representation model and training the adversary for an additional $60$ epochs, it still does not achieve higher accuracies than $50\%$.
    %
    Due to the added noise, the adversary does not overfit on its training set but rather fails to identify any structure in the data.
    
    %
    \Cref{fig:adversarial-learning-curves-failed} shows the learning curves of a failed experiment run where training becomes unstable due to the choice of $N=1000$.
    %
    The adversary learns to counter the representation several times in the alternating training phase.
    %
    These adversary accuracy peaks are always followed by a peak in the de-identification model's validation loss because the representation is worsened for both branches to combat the adversary.
    %
    Due to the combined loss with $\lambda=1$, the best model according to validation loss will often deliver good de-identification results but it may not guarantee robustness to the adversary, even if the concrete adversary test accuracy after the early stopping epoch is low.
    %
    Continued training of the adversary with a frozen representation will allow it to reach higher accuracies.
    
\end{description}

\subsection{Generalization}

\begin{itemize}
    \item Evaluation on the 2016 sight-unseen dataset
\end{itemize}

\subsection{Future Work}
%
Our automatic pseudonymization approach could serve as a data augmentation scheme to be used as a regularizer for de-identification models.
%
Training a model on a combination of raw and pseudonymized data may result in better test scores on the i2b2 test set, possibly beating the state of the art.

%
Our automatic pseudonymization and adversarial learning approaches will most likely be improved by using \ac{elmo} embeddings, which we did not use due to their computation cost.
%
In adversarial learning, it might be possible to tune the $\lambda$ parameter and define a better stopping condition that avoids the unstable characteristics with high values for $N$ in the invariance criterion.
%
A further possible extension is a dynamic noise level in the representation model that depends on the \ac{lstm} output instead of being a trained weight.
%
This might allow using lower amounts of noise for certain inputs while still being robust to the adversary.

%
When more training data from multiple sources becomes available in the future, it will be possible to evaluate our adversarially learned representation against unseen data.
%
Additionally, federated learning and the semi-supervised knowledge transfer approach for de-identification can be reasonably simulated with multiple sources of data.

%TODO zalando concatenate different embeddings \citep{akbik2018contextual}
%TODO other languages

