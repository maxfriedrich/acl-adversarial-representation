% !TeX root=main
% !TeX spellcheck=en_US

\section{De-Identification Model}\label{sec:deidentification-model}
% TODO we don't use character embeddings -- FastText is character-based, GloVe is not
% TODO maybe leave out FastText because it weakens the point?
%
We use a basic bidirectional \ac{lstm}-\ac{crf} model that has been proven to work well in sequence tagging \citep{huang2015bidirectional,lample2016neural} and de-identification \citep{dernoncourt2017identification,liu2017identification}.
%
In addition to pre-trained FastText \citep{bojanowski2016enriching} or GloVe \citep{pennington2014glove} word embeddings, we provide the casing feature from \citet{reimers2017optimal} as an input.
%
The feature maps words to a one-hot representation of their casing (\textit{numeric}, \textit{mainly numeric}, \textit{all lower}, \textit{all upper}, \textit{initial upper}, \textit{contains digit}, or \textit{other}).

%
When trained on the raw i2b2 2014 data, our models achieve \fone scores that are comparable to the state of the art (see \cref{tab:baseline-results}).
%
The casing feature improves GloVe by $0.4$ percentage points.

\begin{table}
    \centering
    \begin{tabular}{ll}
        \toprule
        Model & \fone (\%)\\
        \midrule
        % \Ac{elmo} & $97.74$ \\
        Our FastText & $97.67$ \\
        Our GloVe & $97.62$ \\
        \citet{dernoncourt2017identification} & $97.85$\\
        \citet{lee2017transfer} & $\bm{97.97}$\\
    \end{tabular}
    \caption{Average precision, recall, and \fone scores of our de-identification models on the i2b2 2014 dataset in comparison to the state of the art.}\label{tab:baseline-results}
\end{table}