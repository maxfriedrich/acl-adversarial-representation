% !TeX root=main
% !TeX spellcheck=en_US

\section{Introduction}\label{sec:introduction}
%
In addition to structured medical data, electronic health records contain free-text patient notes that are a rich source of information \citep{jensen2012mining}.
%
Due to privacy and data protection laws, medical records can only be shared and used for research if they are sanitized.
%
De-identification is the task of finding and labeling \ac{phi} in medical text for sanitization.
%
\Ac{phi} includes potentially identifying information such as names, professions, geographic identifiers, dates, and account numbers.
%
The American \ac{hipaa} defines 18 categories of \ac{phi}.

% TODO Why is it important? (enable large-scale studies)

% TODO de-identification as a sequence tagging task like NER

% TODO challenges of de-identification: lexical overlap, misspellings, etc

%
Trying to train a software tool for automatic de-identification leads to a ``chicken and egg problem''~\citep{uzuner2007evaluating}: without a comprehensive training set, an automatic de-identification tool cannot be developed, but without such a tool, it is difficult to share de-identified medical text for research (including for training the tool itself).
%
The standard method of data protection compliant sharing of training data for a de-identification tool requires humans to pseudonymize protected information with substitutes in a document-coherent way.
%
This requires replacing e.g.\ every person or place name with a different name, offsetting dates by a random amount while retaining date intervals, and replacing misspellings with similar misspellings on the pseudonym \cite{uzuner2007evaluating}.

% Why is manual pseudonymization not so great? (time-consuming, coherency)
%
Today, a pseudonymized dataset for de-identification from a single source is publicly available \citep{stubbs2015annotating}.
%
However, de-identification tools trained on the dataset are too specific for the concrete data and do not generalize well to data from other sources~\citep{stubbs2017identification}.
%
If a medical institution instead decides to train a de-identification tool on their raw text data, it is conceivable that the tool would contain traces of the \ac{phi} it was trained with, making it possible for an attacker to recover parts of the training data if the tool itself is shared.
%
To achieve a universal de-identification tool, many medical institutions would have to pool their data.
%
Preparing this data for sharing using the document-coherent pseudonymization approach requires large human effort \citep{dernoncourt2017identification}.

% Why is sharing training data the best solution for a de-identification model?
% TODO automatic pseudonymization as a requirement/precursor for the learned representation instead of describing them as ``two approaches''
%
We introduce two representation approaches to privacy-preserving sharing of medical text that allow training a de-identification tool: an automatic pseudonymization and an adversarially learned private representation.
%
Our approaches still requires humans to annotate \ac{phi} (as this is the training data for the task) but the pseudonymization step is performed by the transformation to the representations.
%
A tool trained on our representations could easily be made publicly available because its parameters cannot contain any protected data, as it is never trained on raw text.
%
Simplifying the de-identification procedure may enable large-scale medical studies that are otherwise too costly.

% TODO present results
% use acronym short forms here (\acs{â€¦}) and explain them later
We train a basic deep learning de-identification model on raw and automatically pseudonymized text as well as on our adversarial private representation.
%

\section{Related Work}\label{sec:related}
%


\subsection{Automatic De-Identification}
%
Analogously to many \ac{nlp} task, the state of the art in de-identification changed from rule-based systems and shallow machine learning approaches like \acp{crf} \citep{uzuner2007evaluating,meystre2010automatic} to deep learning methods \citep{stubbs2017identification, dernoncourt2017identification, liu2017identification} in recent years.

%
Three shared tasks on de-identification were run in 2006 \citep{uzuner2007evaluating}, 2014 \citep{stubbs2015annotating, stubbs2015automated}, and 2016 \citep{stubbs2017identification}.
%
The organizers performed manual pseudonymization on clinical records from a single source to create the datasets for each of the shared tasks.
%
An \fone score of $95\%$ has been suggested as a target for reasonable de-identification systems \citep{stubbs2015automated}.

%
Up to the 2014 shared task, the organizers emphasized that it is unclear if a tool trained on the provided datasets will generalize to medical records from other sources \citep{uzuner2007evaluating,stubbs2015automated}.
%
The 2016 shared task featured a sight-unseen track in which existing systems were evaluated on records from a new data source.
%
The best system achieved an \fone score of only $79\%$, proving that de-identification systems at the time were not able to deliver sufficient performance on completely new data \citep{stubbs2017identification}.

%
\citet{dernoncourt2017identification} first applied \iac{lstm} \citep{hochreiter1997long} model with \iac{crf} output component to de-identification.
%
Using transfer learning from a larger dataset slightly improves performance on the i2b2 2014 dataset \citep{lee2017transfer}.
%
\citet{liu2017identification} achieve state-of-the-art performance in de-identification with a deep learning ensemble and a rule component.

\subsection{Adversarial Representation Learning}
%
Fair representations \citep{zemel2013learning,hamm2015preserving} aim to encode features of raw data that allows it to be used in e.g.\ machine learning algorithms while obfuscating membership in a protected group or other attributes.
%
The \ac{dann} architecture \citep{ganin2016domain} is a deep learning implementation of a three-party game between a representer, classifier, and adversary component.
%
It uses a gradient reversal layer to worsen the representation for the adversary during back-propagation.

%
Although initially conceived for use in domain adaptation, \acp{dann} and similar adversarial deep learning models have recently been used to obfuscate demographics attributes \citep{elazar2018adversarial,li2018towards} from text and subject identity \citep{feutry2018learning} from images.
%
\citet{elazar2018adversarial} warn that continued adversary training with a frozen representation may allow adversaries to break representation privacy.

\section{Dataset and Methods}

\subsection{i2b2 2014 Dataset}
%
The i2b2 2014 dataset~\citep{stubbs2015annotating} was released as part of the 2014 i2b2/UTHealth shared task track 1 and is the largest publicly available dataset for de-identification today.
% TODO update this date / talk about the other dataset if we use it
%
It contains 1304 free-text documents with \ac{phi} annotations.
%

%
The i2b2 dataset uses the 18 categories of \ac{phi} defined by \ac{hipaa} as a starting point for its own set of \ac{phi} categories.
%
In addition to the \ac{hipaa} set of categories, it includes (sub-)categories such as doctor names, professions, states, countries, and ages under 90.

\subsection{De-Identification Model}\label{sec:deidentification-model}
% TODO we don't use character embeddings -- FastText is character-based, GloVe is not
% TODO maybe leave out FastText because it weakens the point?
%
We use a basic bidirectional \ac{lstm}-\ac{crf} model that has been proven to work well in sequence tagging \citep{huang2015bidirectional,lample2016neural} and de-identification \citep{dernoncourt2017identification,liu2017identification}.
%
In addition to pre-trained FastText \citep{bojanowski2016enriching} or GloVe \citep{pennington2014glove} word embeddings, we provide the casing feature from \citet{reimers2017optimal} as an input.
%
The feature maps words to a one-hot representation of their casing (\textit{numeric}, \textit{mainly numeric}, \textit{all lower}, \textit{all upper}, \textit{initial upper}, \textit{contains digit}, or \textit{other}).

%
\cref{tab:deid-hyperparameters} shows our model's hyperparameter configuration that was determined through a random hyperparameter search.

\begin{table}
    \centering
    \begin{tabular}{ll}
     \toprule
     Hyperparameter & Value\\
     \midrule
     Pre-trained embeddings & FastText, GloVe\\
     Casing feature & Yes\\
     Batch size & 32\\
     Number of LSTM layers & 2\\
     LSTM units per layer/dir. & 128\\
     Input embedding dropout & $0.1$\\
     Variational dropout & $0.25$\\
     Dropout after LSTM & $0.5$\\
     Optimizer & Nadam\\
     Gradient norm clipping & $1.0$\\
     \bottomrule
    \end{tabular}
    \caption{Hyperparameter configuration of our de-identification model.}\label{tab:deid-hyperparameters}
\end{table}


\subsection{Automatic Pseudonymization}\label{sec:automatic-pseudonymization}
%
We propose a naive word-level automatic pseudonymization approach that exploits the fact that state-of-the-art de-identification models work on the sentence level and do not rely on document coherency \citep{liu2017identification,dernoncourt2017identification}.
%
Before training, we replace all \ac{phi} tokens with a random choice of a fixed number $N$ of their neighbors in an embedding space, as determined by cosine distance in a pre-computed embedding matrix.
%
% TODO we also shuffle the sentences 

%
Using this approach, the sentence\footnote{\ac{phi} annotations are marked with [brackets]}
%
% TODO maybe find a better example sentence that totally changes meaning?
\begin{quote}
    [James] was admitted to [St. Thomas]
\end{quote}
%
may be replaced by
\begin{quote}
    [Henry] was admitted to [Croix Scott].
\end{quote}
%
While the resulting sentences do not necessarily make sense to a reader (e.g.\ ``Croix Scott'' is not a realistic hospital name), its embedding representation is similar to the original.
%
We train the de-identification model on the transformed data and test it on the raw data.


\subsection{Adversarial Representation}\label{sec:adversarial-representation}
%
Based on the automatic pseudonymization, we propose an adversarially learned sentence representation that is invariant to \textit{any} of the \ac{phi} tokens being replaced with \textit{any} of its $N$ neighbors.
%
Additionally, it adds random noise to representations in way that prohibits building a lookup table for known-plaintext attacks.

\begin{description}
    \item[Architecture]
    %
    Our approach uses a model that is composed of three components: a representation model, the de-identification model from \cref{sec:deidentification-model}, and an adversary.
    %
    An overview of the architecture is shown in \cref{fig:adversarial-model}.
    
    %
    The representation model maps a sequence of word embeddings to an intermediate vector representation sequence.
    %
    The de-identification model receives this representation sequence as an input instead of the original embedding sequence.
    %
    It retains the casing feature as an additional input.
    %
    The representation sequence is also used as one of the two inputs to the adversary that tries to infer information about the original sequence.
    %
    % TODO describe the second input

    \begin{figure}
        \centering
        \input{images/tikz/adversarial}
        \caption[Adversarial model architecture]{%
            Simplified visualization of the adversarial model architecture.
            %
            Sequences of squares denote real-valued vectors, dotted arrows represent possible additional real or fake inputs to the adversary.
            %
            The casing feature that is provided as a second input to the de-identification model is omitted for legibility.}\label{fig:adversarial-model}
    \end{figure}
    
    \item[Adversaries]
    %
    In previous gradient reversal approaches \citep{ganin2016domain,feutry2018learning,elazar2018adversarial}, the learned representation is invariant to some attribute of the input.
    %
    Similarly, our representation should be invariant to small input changes, like a single token being replaced with a neighbor in the embedding space.
    %
    The number of neighbors $N$ controls the privacy properties of the representation.
    
    % TODO Why do we want to share the output representations as well as the representation model?
    %
    Additionally, we need our representation to contain a random element because we want to share the output representations as well as the representation model itself.
    %
    An attacker should not be able to create a lookup table of representations for exact sentences, i.e.\ the representation must be immune to known-plaintext attacks.
    
    %
    To achieve these goals, we use two adversaries that are trained for the following tasks:
    \begin{enumerate}
        \item Given a representation and an embedding sequence, decide if they were obtained from the same sentence.
        \item Given two representation sequences (and their cosine similarities), decide if they were obtained from the same sentence.
    \end{enumerate}
    
    %
    The first adversary's objective is a discriminatory formulation of an inverse representation model and causes representations for similar inputs (replacing any protected token with one of its $N$ neighbors) to be indistinguishable.
    %
    The second adversary's objective causes repeated representation computations for the same sentence to differ by a high enough degree to make it impossible to build a lookup table of representations.
    %
    We obtain the representation sequences for the second adversary from copies of the representation model with shared weights.
    %
    We generate real and fake pairs for adversarial training using the automatic pseudonymization approach presented in \cref{sec:automatic-pseudonymization}, limiting the number of replaced tokens to one per sentence.
    
    %
    The adversaries are implemented as bidirectional \ac{lstm} models.
    %
    We confirmed that this type of model is able to learn the adversarial tasks on random data and raw word embeddings in a preliminary experiment.
    %
    To use the two adversaries in our architecture, we average their outputs.
    
    \item[Representation]
    %
    We use a bidirectional \ac{lstm} model to implement the representation.
    %
    It applies Gaussian noise $\bm{N}$ with zero mean and trainable standard deviations to the input embeddings $\bm{E}$ and output sequence.
    %
    The model learns a standard deviation for each of the input and output dimensions.
    %
    \begin{align}
        \bm{R} = \bm{N}_{\text{out}} + \text{LSTM}(\bm{E} + \bm{N}_{\text{in}})
    \end{align}
    
    %
    In contrast to the automatic pseudonymization approach from \cref{sec:automatic-pseudonymization} that only perturbs \ac{phi} tokens, the representation models in this approach process all tokens to represent them in a new embedding space.
    %
    We evaluate the representation sizes $d \in \{50, 100, 300\}$. 
    
    \item[Training]
    %
    We evaluate two training procedures: \ac{dann} training~\citep{ganin2016domain} and the alternating approach by \citet{feutry2018learning}.
    
    %
    In \ac{dann} training, the three components are trained conjointly, optimizing the sum of losses.
    %
    Training the de-identification model modifies the representation model weights to generate a more meaningful representation for de-identification.
    %
    The adversary gradient is reversed with a gradient reversal layer between the adversary and the representation model in the backward pass, causing the representation to become less meaningful for the adversary.
    
    %
    The training procedure by \citet{feutry2018learning} is shown in \cref{fig:feutry-training}.
    %
    It is composed of three sequential phases:
    %
    \begin{enumerate}
        \item The de-identification and representation models are pre-trained together, optimizing the de-identification loss $l_{\text{deid}}$.
        \item The representation model is frozen and the adversary is pre-trained, optimizing the adversarial loss $l_{\text{adv}}$.
        \item In alternation, for one epoch each:
        \begin{enumerate}
            \item The representation is frozen and both de-identification model and adversary are trained, optimizing their respective losses $l_{\text{deid}}$ and $l_{\text{adv}}$.
            \item The de-identification model and adversary are frozen and the representation is trained, optimizing the combined loss $l_{\text{repr}} = l_{\text{deid}} + \lambda \abs{l_{\text{adv}} - l_{\text{random}}}$. \label{item:repr-training}
        \end{enumerate}
    \end{enumerate}
    
    \begin{figure}
        \centering
        \input{images/tikz/training}
        \caption[Adversarial training procedure]{%
            Visualization of \citeauthor{feutry2018learning}'s training procedure.
            %
            The adversarial model layout follows \cref{fig:adversarial-model}: the representation model is at the bottom, the left branch is the de-identification model and the right branch is the adversary.
            %
            In each step, the thick components are trained while the thin components are frozen.
        }\label{fig:feutry-training}
    \end{figure}
    
    %
    In the first two phases, we monitor the respective validation losses for early stopping to decide at which point the training should move on to the next phase.
    %
    The alternating steps in the third phase each last one training epoch.
    %
    We determine the early stopping epoch using only the combined validation loss (\cref{item:repr-training}).
    
    %
    Gradient reversal is achieved by optimizing the combined representation loss while the adversary weights are frozen.
    %
    The combined loss is motivated by the fact that the adversary performance should be the same as a random guessing model, which is a lower bound for anonymization~\citep{feutry2018learning}.
    %
    The term $\abs{l_{\text{adv}} - l_{\text{random}}}$ approaches $0$ when the adversary performance approaches random guessing\footnote{In the case of binary classification: $L_{\text{random}} = -\log \frac{1}{2} \approx 0.6931$.}.
    %
    $\lambda$ is a weighting factor for the two losses; we select $\lambda=1$.

    \item[Application]
    %
    To apply the model in practice, a central model provider would train the three parts of the model on an initial \ac{phi}-annotated dataset, e.g.\ the i2b2 2014 data.
    %
    This initial training should confirm that the learned representation allows training a de-identification model while being robust to the adversary.
    %
    The model provider would then publish the representation model along with their choice of pre-trained word embeddings.
    %
    Medical institutions would use the representation model to transform their \ac{phi}-labeled data into a private representation, which is then sent back to the central model provider with the respective labels.
    %
    This transformation replaces the manual document-coherent pseudonymization that is typically performed to share training data for de-identification.
    
    %
    The model provider would then update the existing de-identification model or train a new model using all available representation data.
    %
    Periodically, the pipeline of representation model (possibly in a version without additive noise) and de-identification model would be published so it can be used by medical institutions on their unlabeled data.
\end{description}

\subsection{Experiments}
%
We evaluate our approaches, we perform experiments using the i2b2 2014 dataset.
%

\begin{description}
    \item[Preprocessing]
    %
    We apply aggressive tokenization similarly to \citet{liu2017identification}, including splitting on all punctuation marks and mid-word e.g.\ if a number is followed by a word (``25yo'' is split into ``25'', ``yo'') in order to minimize in GloVe out-of-vocabulary tokens.
    %
    We extend spaCy's\footnote{\url{https://spacy.io}} sentence splitting heuristics with additional rules for splitting on multiple lines on whitespace and bulleted list items.
    
    \item[Evaluation]
    %
    In order to compare our results to the state of the art, we use the binary \ac{hipaa} \fone score as our main metric for de-identification performance.
    %
    \citet{dernoncourt2017identification} deem it the most important metric: deciding if an entity is \ac{phi} or not is generally more important than assigning the correct category of \ac{phi}, and only \ac{hipaa} categories of \ac{phi} are required to be removed by American law.
\end{description}

\section{Results}

\subsection{Basic De-Identification Model}
%
When trained on the raw i2b2 2014 data, our models achieve \fone scores that are comparable to \citeauthor{dernoncourt2017identification}'s results (see \cref{tab:baseline-results}).
%
The casing feature improves GloVe by $0.4$ percentage points.

% TODO these are the ``hipaa binary token'' scores
\begin{table}
    \centering
    \begin{tabular}{ll}
        \toprule
        Model & \fone (\%)\\
        \midrule
        % \Ac{elmo} & $97.74$ \\
        Our FastText & $97.67$ \\
        Our GloVe & $97.24$ \\
        Our GloVe + casing & $97.62$ \\
        \addlinespace
        \citeauthor{dernoncourt2017identification} (\ac{lstm}-\ac{crf}) & $97.85$\\
        \citeauthor{liu2017identification} (ensemble + rules) & $\bm{98.27}$\\
        \bottomrule
    \end{tabular}
    \caption{Average precision, recall, and \fone scores of our de-identification models on the i2b2 2014 dataset in comparison to the state of the art.}\label{tab:baseline-results}
\end{table}

\subsection{Automatic Pseudonymization}
%
For both FastText and GloVe, moving training \ac{phi} tokens to random tokens from up to their $N=200$ closest neighbors does not significantly reduce de-identification performance (see \cref{fig:auto-pseudo}).
%
\fone scores for both models drop to around $95\%$ when selecting from $N=500$ neighbors and to around $90\%$ when using $N=1\,000$ neighbors.
%
With $N=100$, the FastText model achieves an \fone score of $96.75\%$ and the GloVe model achieves an \fone score of $96.42\%$.

\begin{figure}
    \centering
    \input{images/plt/automatic-pseudonymization.pgf}
    \caption[De-identification with automatic pseudonymization]{%
        \fone scores of our models when trained on automatically pseudonymized data where \ac{phi} tokens are moved to one of different numbers of neighbors $N$.
        %
        The gray dashed line marks the $95\%$ target \fone score.
}\label{fig:auto-pseudo}
\end{figure}

\subsection{Adversarial Representation}
%
We do not achieve satisfactory results with the conjoint \ac{dann} training procedure: in all cases, our models learn representations that are not sufficiently resistant to the adversary.
%
When training the adversary on the frozen representation for an additional $20$ epochs, it is able to distinguish real from fake input pairs on a test set with accuracies above $80\%$.
%
This confirms the findings by \citet{elazar2018adversarial}.

%
With the training procedure by \citet{feutry2018learning}, we are able to train a representation that allows training a de-identification model while preventing an adversary from learning the adversarial tasks, even with continued training on a frozen representation.

%
\cref{fig:adversarial-deid} (left) shows our de-identification results when using adversarially learned representations.
%
A higher number of neighbors $N$ means a stronger invariance requirement for the representation.
%
For values of $N$ up to $1\,000$, our FastText and GloVe models are able to learn representations that allow training de-identification models that reach the target \fone score of $95\%$.
%
However, training becomes unstable for $N>50$ when using GloVe and $N>500$ when using FastText embeddings.
%
Then, training results in representations that are not robust to the adversary, which is shown in the diagram on the right side.

%
Our choice of representation size $d \in \{50, 100, 300\}$ does not influence de-identifi\-ca\-tion or adversary performance, so we select $d=50$ for further evaluation.
%
For $d=50$ and $N=100$, the FastText model reaches an \fone score of $97.4\%$ and the GloVe model reaches an \fone score of $96.89\%$.

\begin{figure*}
    \centering
    \input{images/plt/adversarial-deid-50.pgf}
    \caption[De-identification with adversarially learned representations]{%
        Left: de-identification \fone scores of our models using an adversarially trained representation with different numbers of neighbors $N$ for the representation invariance requirement.
        %
        Right: mean accuracy on the two adversary tasks.
    }\label{fig:adversarial-deid}
    % TODO the adversary accuracy figure with the lighter color lines is not super clear
\end{figure*}

\section{Discussion}\label{sec:discussion}

\subsection{De-Identification Performance}

\begin{description}
    \item[Baseline De-Identification]
    %
    We find that the choice of GloVe or FastText embeddings does not meaningfully influence de-identification performance.
    %
    \citet{reimers2017optimal} find that GloVe performs better than FastText in their \ac{ner} benchmark.
    %
    This is no contradiction to our result as the i2b2 2014 dataset in all likelihood contains more out-of-vocabulary tokens (that GloVe cannot embed) than their datasets.
    %
    FastText's approach to embedding unknown words (word embeddings are the sum of their subword embeddings) proves useful on datasets with misspellings and ungrammatical text.
    %
    However, FastText beats GloVe only by $0.05$ percentage points on the i2b2 test set.
    %
    The casing feature (which improves GloVe be $0.4$ percentage points in the hyperparameter optimization) makes up for GloVe's missing embeddings for unknown words.
        
    % TODO error analysis with only GloVe + FastText (or only GloVe)
    % TODO explain why we don't beat the Liu et al model (no rules)

    \item[Automatically Pseudonymized Data]
    %
    Our naive automatic word-level pseudonymization approach allows training reasonable de-identification models when selecting from up to $N=500$ neighbors.
    
    % TODO is this discussion?
    %
    It is notable that models that were trained on automatically pseudonymized data outperform the corresponding model that was trained on raw data in \fone score in several categories of \ac{phi}.
    %
    The FastText model beats the respective raw data model in the profession, location, age and contact categories.
    %
    The automatic pseudonymization GloVe model outperforms the raw data model in the name, location, and date categories.
    %
    However, the overall binary \ac{hipaa} \fone scores do not improve when using automatic pseudonymization.
    
    \item[Adversarially Learned Representation]
    %
    Our adversarially trained vector representation that is invariant to word changes allows training reasonable de-identification models when using up to $N=1000$ neighbors as an invariance requirement.
    
    %
    The adversarial de-identification results beat the automatic pseudonymization results because the representation model can act as a task-specific feature extractor.
    %
    Additionally, the representations are more general as they are invariant to word changes.
    %
    The de-identification model is trained on sentences that are augmented to a smaller degree than in our automatic pseudonymization experiment (only one \ac{phi} token is moved to a neighbor in adversarial training instead of all \ac{phi} tokens).
    
    %
    The adversarially trained FastText model beats the FastText model trained on raw data in the contact category. 
    %
    The GloVe model does not beat the corresponding raw data model in any category.
    %
    It is most significantly weaker in the profession category.
\end{description}

\begin{figure*}
    \centering
    \input{images/plt/adversarial-learning-curve-0.pgf}
    \caption{Learning curves of one adversarial experiment run (FastText embeddings, $N=10, d=50$).
        %
        The training is split into three parts: de-identification pre-training, adversary pre-training, and alternating training.
        %
        Dashed lines denote model resets after early stopping.}\label{fig:adversarial-learning-curves}
\end{figure*}

\subsection{Privacy Properties}
%
In this section, we discuss the privacy properties of our approaches.

\begin{description}
    \item[Embeddings]
    %
    When looking up embedding space neighbors for words, it is notable that many FastText neighbors include the original word or parts of it as a subword.
    %
    This is due to FastText's method of using the sum of subword embeddings in embedding calculation.
    %
    For tokens that occur as \ac{phi} in the i2b2 training set, on average $7.37$ of their $N=100$ closest neighbors in the FastText embedding matrix contain the original token as a subword.
    %
    When looking up neighbors using GloVe embeddings, the value is $0.44$.
    %
    This may indicate that FastText requires stronger perturbation (i.e.\ higher $N$) than GloVe to sufficiently obfuscate protected information.
    
    \item[Automatically Pseudonymized Data]
    %
    The word-level pseudonymization may allow an adversary to piece together documents from the shuffled sentences.
    %
    If multiple sentences contain similar pseudonymized identifiers, they will likely come from the same original document, undoing the privacy gain from shuffling training sentences across documents.
    %
    It may be possible to infer the original information using the overlapping neighbor spaces.
    %
    To counter this, we can re-introduce document-level pseudonymization, i.e.\ moving all occurrences of \iac{phi} token to the same neighbor.
    %
    However, we would then also need to detect misspelled names as well as other hints to the actual tokens and transform them similarly to the original, which would add back much of the complexity of manual pseudonymization that we try to avoid.
    
    \item[Adversarially Learned Representation]
    %
    Our adversarial representation empirically satisfies a strong privacy criterion: representations are invariant to \textit{any} protected information token being replaced with \textit{any} of its $N$ neighbors in an embedding space.
    %
    While it is possible for de-identification models to achieve \fone scores of $95\%$ using our adversarially learned representation with up to $N=1000$ neighbors, training becomes unstable for large $N$.
    
    %
    \cref{fig:adversarial-learning-curves} shows a set of typical (successful) learning curves for \citeauthor{feutry2018learning}'s training procedure.
    %
    The representation model and de-identification model are jointly pre-trained in the first phase.
    %
    In the second phase, the adversary is pre-trained to reach a validation accuracy of around $80\%$, which means that the pre-trained representation does not fit our invariance requirements.
    %
    At the beginning of the third training phase, both the de-identification model and the adversary learning curves show an oscillating pattern that is caused by the alternating training of the branches (with frozen representation model) and the representation model (with frozen branches).
    %
    Around 15 epochs into this phase, we find an adequate representation which is indicated by the adversary accuracy being close to the random guessing accuracy of $50\%$.
    %
    The de-identification loss improves through further training.

    %
    When freezing the representation model and training the adversary for an additional $60$ epochs, it still does not achieve higher accuracies than $50\%$ (not shown in the figure).
    %
    Due to the added noise, the adversary does not overfit on its training set but rather fails to identify any structure in the data.    
\end{description}

\subsection{Generalization}

\begin{itemize}
    \item Evaluation on the 2016 sight-unseen dataset
\end{itemize}

\subsection{Future Work}
%
Our automatic pseudonymization approach could serve as a data augmentation scheme to be used as a regularizer for de-identification models.
%
Training a model on a combination of raw and pseudonymized data may result in better test scores on the i2b2 test set, possibly beating the state of the art.

%
In adversarial learning, it might be possible to tune the $\lambda$ parameter and define a better stopping condition that avoids the unstable characteristics with high values for $N$ in the invariance criterion.
%
A further possible extension is a dynamic noise level in the representation model that depends on the \ac{lstm} output instead of being a trained weight.
%
This might allow using lower amounts of noise for certain inputs while still being robust to the adversary.

% TODO or maybe it is possible now if we get the sight-unseen data
%
When more training data from multiple sources becomes available in the future, it will be possible to evaluate our adversarially learned representation against unseen data.
%
Additionally, federated learning and the semi-supervised knowledge transfer approach for de-identification can be reasonably simulated with multiple sources of data.

%TODO mention elmo \citep{peters2018deep} here
%TODO zalando concatenate different embeddings \citep{akbik2018contextual}
%TODO other languages

\section{Conclusions}\label{sec:conclusions}
%
Privacy laws require medical text to be de-identified before it is shared.
%
De-identifi\-ca\-tion is time-consuming and costly when performed by humans, which motivates the creation of automatic de-identification tools.
%
Automatic de-identification requires training data, which is typically created by substituting all protected information from raw medical records.
%
Today's de-identification tools fail on unseen data.
%
A training set from multiple sources is required to train more general de-identification tools.
%
We evaluated approaches to sharing training data for de-identification that require lower human effort than the existing approach of document-coherent pseudonymization.

%
As a precursor to our data sharing approaches, we developed a baseline deep learning model for de-identification that does rely on explicit character features.
%
It uses word embeddings as well as a casing feature as inputs.
%
On the i2b2 2014 test set, the model reaches an \fone score of $97.74\%$, a near-state-of-the-art result.

%
Our automatic pseudonymization approach replaces protected information with neighboring words in an embedding space.
%
A model trained on this augmented data with $N=100$ neighbors loses around one percentage point in \fone score when compared to the raw data baseline, scoring $96.75\%$.

%
We presented an adversarial learning based private representation of medical text that is invariant to any protected information word being replaced with any of its embedding space neighbors and contains a random element.
%
The representation allows training a de-identification model while being robust to adversaries trying to re-identify protected information or building a lookup table of representations.
%
We extended existing adversarial representation learning approaches by using two adversaries that discriminate real from fake sequence pairs with an additional sequence input.
%
Using the adversarially learned representation, de-identification models reach an \fone score of $97.4\%$, which is closer to the raw data baseline than to the automatic pseudonymization score.
%
The representation acts as a task-specific feature extractor.
%
For an invariance criterion of up to $N=50$ (GloVe) or $N=500$ (FastText) neighbors, training is stable and adversaries cannot beat the random guessing accuracy of $50\%$.

%
Our approaches, especially the adversarially trained representation, allow cost-effective private sharing of training data for de-identification.
%
Better de-identification tools could help enable large-scale medical studies that improve public health.
