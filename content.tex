% !TeX root=main
% !TeX spellcheck=en_US

% TODO general: is it ``medical'' or ``clinical'' text?

\section{Introduction}\label{sec:introduction}
%
\Acp{ehr} are a rich source of information that could potentially be used in large-scale medical research \citep{botsis2010secondary, birkhead2015uses, cowie2017electronic}.
%
In addition to structured medical data, \acp{ehr} contain free-text patient notes that are a rich source of information \citep{jensen2012mining}.
%
Due to privacy and data protection laws, medical records can only be shared and used for research if they are sanitized.
%
De-identification is the task of finding and labeling \ac{phi} in medical text for sanitization.
%
\Ac{phi} includes potentially identifying information such as names, geographic identifiers, dates, and account numbers.
%
The American \acl{hipaa} (\acs{hipaa}, \citeyear{usa1996hipaa}) defines 18 categories of \ac{phi}.

%
Trying to create an automatic classifier for de-identification leads to a ``chicken and egg problem''~\citep{uzuner2007evaluating}: without a comprehensive training set, an automatic de-identification classifier cannot be developed, but without such a classifier, it is difficult to share de-identified medical text for research (including for training the classifier itself).
%
The standard method of data protection compliant sharing of training data for a de-identification classifier requires humans to pseudonymize protected information with substitutes in a document-coherent way.
%
This includes replacing e.g.\ every person or place name with a different name, offsetting dates by a random amount while retaining date intervals, and replacing misspellings with similar misspellings on the pseudonym \cite{uzuner2007evaluating}.

% Why is manual pseudonymization not so great? (time-consuming, coherency)
% Why is sharing training data the best solution for a de-identification model?
%
Today, a pseudonymized dataset for de-identification from a single source, the i2b2 2014 dataset, is publicly available \citep{stubbs2015annotating}.
%
However, de-identification classifiers trained on this dataset do not generalize well to data from other sources~\citep{stubbs2017identification}.
%
To achieve a universal de-identification classifier, many medical institutions would have to pool their data.
%
Preparing this data for sharing using the document-coherent pseudonymization approach requires large human effort \citep{dernoncourt2017identification}.

%
We introduce an adversarially learned representation of medical text that allows privacy-preserving sharing of training data for a de-identification classifier.
%
Our approach still requires humans to annotate \ac{phi} (as this is the training data for the de-identification task) but the pseudonymization step is performed by the transformation to the representation.
%
A classifier trained on our representation cannot contain any protected data, as it is never trained on raw text.

\section{Related Work}\label{sec:related}
%
In this section, we present related work regarding de-identification and adversarial representation learning.

\subsection{Automatic De-Identification}
%
Analogously to many \ac{nlp} task, the state of the art in de-identification changed from rule-based systems and shallow machine learning approaches like \acp{crf} \citep{uzuner2007evaluating,meystre2010automatic} to deep learning methods \citep{stubbs2017identification, dernoncourt2017identification, liu2017identification} in recent years.

%
Three shared tasks on de-identification were run in 2006 \citep{uzuner2007evaluating}, 2014 \citep{stubbs2015automated}, and 2016 \citep{stubbs2017identification}.
%
The organizers performed manual pseudonymization on clinical records from a single source to create the datasets for each of the tasks.
%
An \fone score of $95\%$ has been suggested as a target for reasonable de-identification systems \citep{stubbs2015automated}.

%
\citet{dernoncourt2017identification} first applied \iac{lstm} \citep{hochreiter1997long} model with \iac{crf} output component to de-identification.
%
Using transfer learning from a larger dataset slightly improves performance on the i2b2 2014 dataset \citep{lee2017transfer}.
%
\citet{liu2017identification} achieve state-of-the-art performance in de-identification by combining a deep learning ensemble with a rule component.

%
Up to the 2014 shared task, the organizers emphasized that it is unclear if a system trained on the provided datasets will generalize to medical records from other sources \citep{uzuner2007evaluating,stubbs2015automated}.
%
The 2016 shared task featured a sight-unseen track in which de-identification systems were evaluated on records from a new data source.
%
The best system achieved an \fone score of $79\%$, proving that systems at the time were not able to deliver sufficient performance on completely new data \citep{stubbs2017identification}.

\subsection{Adversarial Representation Learning}
%
Fair representations \citep{zemel2013learning,hamm2015preserving} aim to encode features of raw data that allows it to be used in e.g.\ machine learning algorithms while obfuscating membership in a protected group or other sensitive attributes.
%
The \ac{dann} architecture \citep{ganin2016domain} is a deep learning implementation of a three-party game between a representer, classifier, and adversary component.
%
It uses a gradient reversal layer to worsen the representation for the adversary during back-propagation.

%
Although initially conceived for use in domain adaptation, \acp{dann} and similar adversarial deep learning models have recently been used to obfuscate demographics attributes from text \citep{elazar2018adversarial,li2018towards} and subject identity \citep{feutry2018learning} from images.
%
\citet{elazar2018adversarial} warn that continued adversary training with a frozen representation may allow adversaries to break representation privacy.

\section{Dataset and Methods}
%
In this section, we describe the i2b2 2014 dataset as well as our experimental approaches.

\subsection{i2b2 2014 Dataset}
%
The i2b2 2014 dataset~\citep{stubbs2015annotating} was released as part of the 2014 i2b2/UTHealth shared task track 1 and is the largest publicly available dataset for de-identification today.
%
It contains 1304 free-text documents with \ac{phi} annotations.
%

%
The i2b2 dataset uses the 18 categories of \ac{phi} defined by \ac{hipaa} as a starting point for its own set of \ac{phi} categories.
%
In addition to the \ac{hipaa} set of categories, it includes (sub-)categories such as doctor names, professions, states, countries, and ages under 90.

\subsection{De-Identification Model}\label{sec:deidentification-model}
%
We use a basic bidirectional \ac{lstm}-\ac{crf} model that has been proven to work well in sequence tagging \citep{huang2015bidirectional,lample2016neural} and de-identification \citep{dernoncourt2017identification,liu2017identification}.
%
We only use pre-trained FastText \citep{bojanowski2016enriching} or GloVe \citep{pennington2014glove} word embeddings, not explicit character embeddings, as we suspect these may allow easy re-identification of private information if used in shared representations.
% TODO does this even make sense? In the baseline model, the data is public, and in the adversarial representation model, the representation should be able to obscure the character features
%
Instead of learned character features, we provide the casing feature from \citet{reimers2017optimal} as an additional input.
%
The feature maps words to a one-hot representation of their casing (\textit{numeric}, \textit{mainly numeric}, \textit{all lower}, \textit{all upper}, \textit{initial upper}, \textit{contains digit}, or \textit{other}).

%
\Cref{tab:deid-hyperparameters} shows our model's hyperparameter configuration that was determined through a random hyperparameter search.

\begin{table}
    \centering
    \begin{tabular}{ll}
     \toprule
     Hyperparameter & Value\\
     \midrule
     Pre-trained embeddings & FastText, GloVe\\
     Casing feature & Yes\\
     Batch size & 32\\
     Number of LSTM layers & 2\\
     LSTM units per layer/dir. & 128\\
     Input embedding dropout & $0.1$\\
     Variational dropout & $0.25$\\
     Dropout after LSTM & $0.5$\\
     Optimizer & Nadam\\
     Gradient norm clipping & $1.0$\\
     \bottomrule
    \end{tabular}
    \caption{Hyperparameter configuration of our de-identification model.}\label{tab:deid-hyperparameters}
\end{table}


\subsection{Automatic Pseudonymization}\label{sec:automatic-pseudonymization}
%
We introduce a naive word-level automatic pseudonymization approach that exploits the fact that state-of-the-art de-identification models \citep{liu2017identification,dernoncourt2017identification} as well as our basic de-identification model work on the sentence level and do not rely on document coherency.
%
Before training, we shuffle the training sentences and replace all \ac{phi} tokens with a random choice of a fixed number $N$ of their closest neighbors in an embedding space (including the token itself), as determined by cosine distance in a pre-computed embedding matrix.

%
Using this approach, the sentence\footnote{\ac{phi} annotations are marked with [brackets]}
%
% TODO maybe find a better example sentence that totally changes meaning?
\begin{quote}
    [James] was admitted to [St. Thomas]
\end{quote}
%
may be replaced by
\begin{quote}
    [Henry] was admitted to [Croix Scott].
\end{quote}
%
While the resulting sentences do not necessarily make sense to a reader (e.g.\ ``Croix Scott'' is not a realistic hospital name), its embedding representation is similar to the original.
%
We train our de-identification model on the transformed data and test it on the raw data.
%
The number of neighbors $N$ controls the privacy properties of the approach: $N = 1$ means no pseudonymization; setting $N$ to the number of rows in a precomputed embedding matrix delivers perfect anonymization but the resulting data may be worthless for training a de-identification model.

\subsection{Adversarial Representation}\label{sec:adversarial-representation}

\begin{figure}
    \centering
    \input{images/tikz/adversarial}
    \caption[Adversarial model architecture]{%
        Simplified visualization of the adversarial model architecture.
        %
        Sequences of squares denote real-valued vectors, dotted arrows represent possible additional real or fake inputs to the adversary.
        %
        The casing feature that is provided as a second input to the de-identification model is omitted for legibility.}\label{fig:adversarial-model}
\end{figure}

%
We introduce a new data sharing approach that is based on an adversarially learned private representation and improves on the pseudonymization from \cref{sec:automatic-pseudonymization}.
%
After training the representation on an initial publicly available dataset, e.g.\ the i2b2 2014 data, a central model provider shares the frozen representation model with participating medical institutions.
%
They transform their \ac{phi}-labeled data into the representation, which is then pooled into a new public dataset for de-identification.
%
Periodically, the pipeline consisting of the representation model and a trained de-identification model can be published to be used by medical institutions on their unlabeled data.

%
Since both the representation model and the resulting representations are shared in this scenario, our representation procedure is required to prevent two attacks:
%
\begin{enumerate}[label=A\arabic*.,ref=A\arabic*]
    \item Learning an inverse representation model that transforms representations back to original sentences containing \ac{phi}.\label{item:attack1}
    \item Building a lookup table of inputs and their exact representations that can be used in known plaintext attacks.\label{item:attack2}
\end{enumerate}

\subsubsection{Architecture}
%
Our approach uses a model that is composed of three components: a representation model, the de-identification model from \cref{sec:deidentification-model}, and an adversary.
%
An overview of the architecture is shown in \cref{fig:adversarial-model}.

%
The representation model maps a sequence of word embeddings to an intermediate vector representation sequence.
%
The de-identification model receives this representation sequence as an input instead of the original embedding sequence.
%
It retains the casing feature as an auxiliary input.
%
The adversary has two inputs: the representation sequence and an additional embedding or representation sequence.

\subsubsection{Representation}
%
To protect against \ref{item:attack1}, our representation must be invariant to small input changes, like a single \ac{phi} token being replaced with a neighbor in the embedding space.
%
Again, the number of neighbors $N$ controls the privacy level of the representation.

%
To protect against \ref{item:attack2}, we add a random element to the representation that makes repeated transformations of one sentence indistinguishable from representations of similar input sentences.

%
We use a bidirectional \ac{lstm} model to implement the representation.
%
It applies Gaussian noise $\bm{N}$ with zero mean and trainable standard deviations to the input embeddings $\bm{E}$ and the output sequence.
%
The model learns a standard deviation for each of the input and output dimensions.
%
\begin{align}
\bm{R} = \bm{N}_{\text{out}} + \text{LSTM}(\bm{E} + \bm{N}_{\text{in}})
\end{align}

%
In contrast to the automatic pseudonymization approach from \cref{sec:automatic-pseudonymization} that only perturbs \ac{phi} tokens, the representation models in this approach process all tokens to represent them in a new embedding space.
%
We evaluate the representation sizes $d \in \{50, 100, 300\}$. 

\subsubsection{Adversaries}
%
We use two adversaries that are trained on tasks that directly follow from \ref{item:attack1} and \ref{item:attack2}:
\begin{enumerate}[label=T\arabic*.,ref=T\arabic*]
    \item Given a representation and an embedding sequence, decide if they were obtained from the same sentence.
    \item Given two representation sequences (and their cosine similarities), decide if they were obtained from the same sentence.
\end{enumerate}

%
We obtain the representation sequences for the second adversary from a copy of the representation model with shared weights.
%
We generate real and fake pairs for adversarial training using the automatic pseudonymization approach presented in \cref{sec:automatic-pseudonymization}, limiting the number of replaced \ac{phi} tokens to one per sentence.

%
The adversaries are implemented as bidirectional \ac{lstm} models.
%
We confirmed that this type of model is able to learn the adversarial tasks on random data and raw word embeddings in a preliminary experiment.
%
To use the two adversaries in our architecture, we average their outputs.

 \begin{figure}
    \centering
    \input{images/tikz/training}
    \caption[Adversarial training procedure]{%
        Visualization of \citeauthor{feutry2018learning}'s three-part training procedure.
        %
        The adversarial model layout follows \cref{fig:adversarial-model}: the representation model is at the bottom, the left branch is the de-identification model and the right branch is the adversary.
        %
        In each step, the thick components are trained while the thin components are frozen.
    }\label{fig:feutry-training}
\end{figure}

\subsubsection{Training}
%
We evaluate two training procedures: \ac{dann} training~\citep{ganin2016domain} and the three-part procedure from \citet{feutry2018learning}.

%
In \ac{dann} training, the three components are trained conjointly, optimizing the sum of losses.
%
Training the de-identification model modifies the representation model weights to generate a more meaningful representation for de-identification.
%
The adversary gradient is reversed with a gradient reversal layer between the adversary and the representation model in the backward pass, causing the representation to become less meaningful for the adversary.

%
The training procedure by \citet{feutry2018learning} is shown in \cref{fig:feutry-training}.
%
It is composed of three phases:
%
\begin{enumerate}[label=P\arabic*.,ref=P\arabic*]
    \item The de-identification and representation models are pre-trained together, optimizing the de-identification loss $l_{\text{deid}}$.
    \item The representation model is frozen and the adversary is pre-trained, optimizing the adversarial loss $l_{\text{adv}}$.
    \item In alternation, for one epoch each:
    \begin{enumerate}
        \item The representation is frozen and both de-identification model and adversary are trained, optimizing their respective losses $l_{\text{deid}}$ and $l_{\text{adv}}$.
        \item The de-identification model and adversary are frozen and the representation is trained, optimizing the combined loss
        \begin{align}
            l_{\text{repr}} = l_{\text{deid}} + \lambda \abs{l_{\text{adv}} - l_{\text{random}}}
        \end{align}
        \label{item:repr-training}
    \end{enumerate}
\end{enumerate}

%
In each of the first two phases, we monitor the respective validation loss for early stopping to decide at which point the training should move on to the next phase.
%
The alternating steps in the third phase each last one training epoch.
%
We determine the early stopping time for the third phase using only the combined validation loss from \ref{item:repr-training}.

%
Gradient reversal is achieved by optimizing the combined representation loss while the adversary weights are frozen.
%
The combined loss is motivated by the fact that the adversary performance should be the same as a random guessing model, which is a lower bound for anonymization~\citep{feutry2018learning}.
%
The term $\abs{l_{\text{adv}} - l_{\text{random}}}$ approaches $0$ when the adversary performance approaches random guessing\footnote{In the case of binary classification: $L_{\text{random}} = -\log \frac{1}{2}$.}.
%
$\lambda$ is a weighting factor for the two losses; we select $\lambda=1$.

\subsection{Experiments}
%
To evaluate our approaches, we perform experiments using the i2b2 2014 dataset.

\textbf{Preprocessing:}
%
We apply aggressive tokenization similarly to \citet{liu2017identification}, including splitting on all punctuation marks and mid-word e.g.\ if a number is followed by a word (``25yo'' is split into ``25'', ``yo'') in order to minimize in GloVe out-of-vocabulary tokens.
%
We extend spaCy's\footnote{\url{https://spacy.io}} sentence splitting heuristics with additional rules for splitting on multiple lines on whitespace and bulleted list items.

\textbf{Deep Learning Models:}
%
We use the Keras framework \citep{chollet2015keras} with the TensorFlow backend \citep{abadi2015tensorflow} to implement our deep learning models.
%
%We use \iac{crf} layer implementation from the Keras community contributions repository\footnote{\url{https://github.com/keras-team/keras-contrib}}.

\textbf{Evaluation:}
%
In order to compare our results to the state of the art, we use the token-based binary \ac{hipaa} \fone score as our main metric for de-identification performance.
%
\citet{dernoncourt2017identification} deem it the most important metric: deciding if an entity is \ac{phi} or not is generally more important than assigning the correct category of \ac{phi}, and only \ac{hipaa} categories of \ac{phi} are required to be removed by American law.
%
We perform evaluation with the official evaluation script\footnote{\url{https://github.com/kotfic/i2b2\_evaluation\_scripts}}.

\section{Results}
%
In this section, we present our experiment results.

\subsection{Basic De-Identification Model}
%
When trained on the raw i2b2 2014 data, our models achieve \fone scores that are comparable to \citeauthor{dernoncourt2017identification}'s results (see \cref{tab:baseline-results}).
%
The casing feature improves GloVe by $0.4$ percentage points.

\begin{table}
    \centering
    \begin{tabular}{ll}
        \toprule
        Model & \fone (\%)\\
        \midrule
        % \Ac{elmo} & $97.74$ \\
        Our FastText & $97.67$ \\
        Our GloVe & $97.24$ \\
        Our GloVe + casing & $97.62$ \\
        \addlinespace
        \citeauthor{dernoncourt2017identification} (\ac{lstm}-\ac{crf}) & $97.85$\\
        \citeauthor{liu2017identification} (ensemble + rules) & $\bm{98.27}$\\
        \bottomrule
    \end{tabular}
    \caption{Binary \ac{hipaa} \fone scores of our de-identification models on the i2b2 2014 test set in comparison to the state of the art.}\label{tab:baseline-results}
\end{table}

\subsection{Automatic Pseudonymization}
%
For both FastText and GloVe, moving training \ac{phi} tokens to random tokens from up to their $N=200$ closest neighbors does not significantly reduce de-identification performance (see \cref{fig:auto-pseudo}).
%
\fone scores for both models drop to around $95\%$ when selecting from $N=500$ neighbors and to around $90\%$ when using $N=1\,000$ neighbors.
%
With $N=100$, the FastText model achieves an \fone score of $96.75\%$ and the GloVe model achieves an \fone score of $96.42\%$.

\begin{figure}
    \centering
    \input{images/plt/automatic-pseudonymization.pgf}
    \caption[De-identification with automatic pseudonymization]{%
        \fone scores of our models when trained on automatically pseudonymized data where \ac{phi} tokens are moved to one of different numbers of neighbors $N$.
        %
        The gray dashed line marks the $95\%$ target \fone score.
}\label{fig:auto-pseudo}
\end{figure}

\subsection{Adversarial Representation}
%
We do not achieve satisfactory results with the conjoint \ac{dann} training procedure: in all cases, our models learn representations that are not sufficiently resistant to the adversary.
%
When training the adversary on the frozen representation for an additional $20$ epochs, it is able to distinguish real from fake input pairs on a test set with accuracies above $80\%$.
%
This confirms the findings by \citet{elazar2018adversarial}.

%
With the three-part training procedure, we are able to learn a representation that allows training a de-identification model while preventing an adversary from learning the adversarial tasks, even with continued training on a frozen representation.

%
\Cref{fig:adversarial-deid} (left) shows our de-identification results when using adversarially learned representations.
%
A higher number of neighbors $N$ means a stronger invariance requirement for the representation.
%
For values of $N$ up to $1\,000$, our FastText and GloVe models are able to learn representations that allow training de-identification models that reach the target \fone score of $95\%$.
%
However, training becomes unstable for $N>500$: at this point, the adversary is able to beat the representation when trained for an additional $50$ epochs (\cref{fig:adversarial-deid} right).

%
Our choice of representation size $d \in \{50, 100, 300\}$ does not influence de-identifi\-ca\-tion or adversary performance, so we select $d=50$ for further evaluation.
%
For $d=50$ and $N=100$, the FastText model reaches an \fone score of $97.4\%$ and the GloVe model reaches an \fone score of $96.89\%$.

\begin{figure*}
    \centering
    \input{images/plt/adversarial-deid-50.pgf}
    \caption[De-identification with adversarially learned representations]{%
        Left: de-identification \fone scores of our models using an adversarially trained representation with different numbers of neighbors $N$ for the representation invariance requirement.
        %
        Right: mean adversary accuracy when trained on the frozen representation for an additional $50$ epochs.
    }\label{fig:adversarial-deid}
\end{figure*}

\section{De-Identification Performance}

\textbf{Baseline De-Identification:}
%
We find that the choice of GloVe or FastText embeddings does not meaningfully influence de-identification performance.
%
FastText's approach to embedding unknown words (word embeddings are the sum of their subword embeddings) should intuitively prove useful on datasets with misspellings and ungrammatical text.
%
However, when using the additional casing feature, FastText beats GloVe only by $0.05$ percentage points on the i2b2 test set.
%
In this task, the casing feature makes up for GloVe's inability to embed unknown words.

%
\citet{liu2017identification} use a deep learning ensemble in combination with hand-crafted rules to achieve state-of-the-art results de-identification.
%
Our model's scores are similar to the previous state of the art, a bidirectional \ac{lstm}-\ac{crf} model with character features \citep{dernoncourt2017identification}.

\textbf{Automatically Pseudonymized Data:}
%
Our naive automatic word-level pseudonymization approach allows training reasonable de-identification models when selecting from up to $N=500$ neighbors.
%
There is almost no decrease in \fone score for up to $N = 20$ neighbors for both the FastText and GloVe model.

\textbf{Adversarially Learned Representation:}
%
Our adversarially trained vector representation that is invariant to word changes allows training reasonable de-identification models (\fone scores above $95\%$) when using up to $N=1\,000$ neighbors as an invariance requirement.
%
The adversarial representation results beat the automatic pseudonymization results because the representation model can act as a task-specific feature extractor.
%
Additionally, the representations are more general as they are invariant to word changes.
%
The de-identification model is trained on sentences that are augmented to a smaller degree than in our automatic pseudonymization experiment (only one \ac{phi} token is moved to a neighbor in adversarial training instead of all \ac{phi} tokens).

\section{Privacy Properties}

\textbf{Embeddings:}
%
When looking up embedding space neighbors for words, it is notable that many FastText neighbors include the original word or parts of it as a subword.
%
This is due to FastText's method of using the sum of subword embeddings in embedding calculation.
%
For tokens that occur as \ac{phi} in the i2b2 training set, on average $7.37$ of their $N=100$ closest neighbors in the FastText embedding matrix contain the original token as a subword.
%
When looking up neighbors using GloVe embeddings, the value is $0.44$.
%
This may indicate that FastText requires stronger perturbation (i.e.\ higher $N$) than GloVe to sufficiently obfuscate protected information.

\textbf{Automatically Pseudonymized Data:}
%
The word-level pseudonymization may allow an adversary to piece together documents from the shuffled sentences.
%
If multiple sentences contain similar pseudonymized identifiers, they will likely come from the same original document, undoing the privacy gain from shuffling training sentences across documents.
%
It may be possible to infer the original information using the overlapping neighbor spaces.
%
To counter this, we can re-introduce document-level pseudonymization, i.e.\ moving all occurrences of \iac{phi} token to the same neighbor.
%
However, we would then also need to detect misspelled names as well as other hints to the actual tokens and transform them similarly to the original, which would add back much of the complexity of manual pseudonymization that we try to avoid.

\textbf{Adversarially Learned Representation:}
%
Our adversarial representation empirically satisfies a strong privacy criterion: representations are invariant to \textit{any} protected information token being replaced with \textit{any} of its $N$ neighbors in an embedding space.
%
When freezing the representation model from an experiment run using up to $N = 500$ neighbors and training the adversary for an additional $50$ epochs, it still does not achieve higher-than-chance accuracies.
%
Due to the additive noise, the adversary does not overfit on its training set but rather fails to identify any structure in the data.

%
In the case of $N = 1\,000$ neighbors, the representation never becomes stable in the alternating training phase.
%
The adversary is always able to beat the representation.

\section{Conclusions \& Future Work}
\todo[inline]{Make clearer what the novel contribution is}
%
Privacy laws require medical text to be de-identified before it is shared.
%
De-identifi\-ca\-tion is time-consuming and costly when performed by humans, which motivates the creation of automatic de-identification classifiers.
%
Automatic de-identification requires training data, which is typically created by substituting all protected information from raw medical records.
%
Today's de-identification classifiers fail on unseen data.
%
A training set from multiple sources is required to train more general de-identification classifiers.
%
We introduced a new approach to sharing training data for de-identification that requires lower human effort than the existing approach of document-coherent pseudonymization.

%
As precursors to our adversarial representation approach, we developed a baseline deep learning model for de-identification that does rely on explicit character features as well as an automatic word-level pseudonymization approach.
%
A model trained on our automatically pseudonymized data with $N=100$ neighbors loses around one percentage point in \fone score when compared to the raw data baseline, scoring $96.75\%$ on the i2b2 2014 test set.

%
We presented an adversarial learning based private representation of medical text that is invariant to any \ac{phi} word being replaced with any of its embedding space neighbors and contains a random element.
%
The representation allows training a de-identification model while being robust to adversaries trying to re-identify protected information or building a lookup table of representations.
%
We extended existing adversarial representation learning approaches by using two adversaries that discriminate real from fake sequence pairs with an additional sequence input.
%
Using the adversarially learned representation, de-identification models reach an \fone score of $97.4\%$, which is closer to the raw data baseline than to the automatic pseudonymization score.
%
The representation acts as a task-specific feature extractor.
%
For an invariance criterion of up to $N=500$ neighbors, training is stable and adversaries cannot beat the random guessing accuracy of $50\%$.

%
Our adversarial representation approach allows cost-effective private sharing of training data for de-identification.
%
Better de-identification classifiers could help enable large-scale medical studies that improve public health.

%
\textbf{Future Work:} The automatic pseudonymization approach could serve as a data augmentation scheme to be used as a regularizer for de-identification models.
%
Training a model on a combination of raw and pseudonymized data may result in better test scores on the i2b2 test set, possibly beating the state of the art.

%
In adversarial learning with the three-part training procedure, it might be possible to tune the $\lambda$ parameter and define a better stopping condition that avoids the unstable characteristics with high values for $N$ in the invariance criterion.
%
A further possible extension is a dynamic noise level in the representation model that depends on the \ac{lstm} output instead of being a trained weight.
%
This might allow using lower amounts of noise for certain inputs while still being robust to the adversary.

%TODO Mention other embedding schemes, e.g. elmo \citep{peters2018deep}
% Contextual word embeddings such as ELMo \citep{peters2018deep} ...

%
When more training data from multiple sources becomes available in the future, it will be possible to evaluate our adversarially learned representation against unseen data.

